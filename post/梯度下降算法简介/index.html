<!DOCTYPE html>
<html lang="zh-cn" data-theme="light"><head>
    <meta name="google-site-verification" content="kw1N-Xm6qEr1c9PGuRd0U_T6DXkw_EHsLyz5LpuDDv8" />
    <meta name="msvalidate.01" content="EE98205D30806C22C519683EFC53E9BA" />
    <meta name="baidu-site-verification" content="iPC3wUcQLL" />
    <title>  梯度下降算法 </title>
    <meta charset="utf-8"><meta name="generator" content="Hugo 0.85.0" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
    <meta name="description" itemprop="description"
        content=" 梯度下降算法 ">
    <meta name="keywords" itemprop="keywords"
        content=" [梯度下降算法] ">
    <base href="https://gaoshanwomeng.github.io/">
    <link rel="shortcut icon" href="https://gaoshanwomeng.github.io/favicons//favicon.ico" type="image/x-icon">
    <link rel="icon" type="image/png" sizes="32x32" href="https://gaoshanwomeng.github.io/favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://gaoshanwomeng.github.io/favicons/favicon-16x16.png">
    <link rel="canonical" href="https://gaoshanwomeng.github.io/post/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B/">
    <link rel="stylesheet" type="text/css" href="https://at.alicdn.com/t/font_2450869_iypnqhtzjei.css">
    
    
    
    
    
    
    
    
    <link rel="stylesheet" href="https://gaoshanwomeng.github.io/css/style.min.ccc955d867b53b3b19b6e1dd57cdeea976019ba2168b4e75fe7822acda296f77.css" integrity=""
        type="text/css">
</head><body>
<div class="main animated fadeInDown">
  <div class="toc sub-container">
    <div class="toc-header">
        <span>目录</span>
        <span id="read-percentage"></span>
    </div>
    <ul class="toc-h3"><li>
                    <a href="https://gaoshanwomeng.github.io/post/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B/#1%e7%ae%97%e6%b3%95%e6%80%9d%e6%83%b3" class="toc-link">1.算法思想</a>
                </li>
                <li>
                    <a href="https://gaoshanwomeng.github.io/post/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B/#2%e7%ae%97%e6%b3%95%e5%85%ac%e5%bc%8f" class="toc-link">2.算法公式</a>
                </li>
                <li>
                    <a href="https://gaoshanwomeng.github.io/post/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B/#3%e7%ae%97%e6%b3%95%e5%ba%94%e7%94%a8" class="toc-link">3.算法应用</a>
                </li>
                
                        <ul class="toc-h4"><li>
                    <a href="https://gaoshanwomeng.github.io/post/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B/#1%e5%9c%ba%e6%99%af%e5%88%86%e6%9e%90" class="toc-link">1.场景分析</a>
                </li>
                <li>
                    <a href="https://gaoshanwomeng.github.io/post/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B/#2%e6%b1%82%e8%a7%a3%e6%80%9d%e8%b7%af" class="toc-link">2.求解思路</a>
                </li>
                <li>
                    <a href="https://gaoshanwomeng.github.io/post/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B/#3%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0" class="toc-link">3.代码实现</a>
                </li>
                </div><div class="single-post container">
    <div class="post">
      <div class="header">
        <span class="title">梯度下降算法</span>
        
        <div class="info">
          <span>📅 2020-06-18</span>
          <span>👦 Tomtom Young</span>
          <span>📖 2229字</span>
          <span>⏱ 5分钟</span>
        </div>
        
      </div>
      <div class="content markdown-body">
        <p>梯度下降（gradient descent）在机器学习中应用十分的广泛，不论是在线性回归还是Logistic回归中，它的主要目的是通过迭代找到目标函数的最小值，或者收敛到最小值。</p>
<h3 id="1算法思想">1.算法思想</h3>
<p>梯度下降法的基本思想可以类比为一个下山的过程。
假设这样一个场景：一个人被困在山上，需要从山上下来(找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低；因此，下山的路径就无法确定，必须利用自己周围的信息一步一步地找到下山的路。这个时候，便可利用梯度下降算法来帮助自己下山。怎么做呢，首先以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着下降方向走一步，然后又继续以当前位置为基准，再找最陡峭的地方，再走直到最后到达最低处；同理上山也是如此，只是这时候就变成梯度上升算法了。</p>
<h3 id="2算法公式">2.算法公式</h3>
<p>$$\theta ^{1}=\theta ^{0}-\alpha \nabla J\left( \theta \right) $$</p>
<ol>
<li>
<p>$\alpha$ 在梯度下降算法中被称作为学习率或者步长，意味着我们可以通过α来控制每一步走的距离。我们保证不要步子跨的太大，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！</p>
</li>
<li>
<p>$\nabla J\left( \theta \right) $ 指的是梯度，梯度是微积分中一个很重要的概念：</p>
<ul>
<li>在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率</li>
<li>在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向</li>
</ul>
<p>这也就说明了为什么我们需要千方百计的求取梯度！我们需要到达山底，就需要在每一步观测到此时最陡峭的地方，梯度就恰巧告诉了我们这个方向。梯度的方向是函数在给定点上升最快的方向，那么梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的，因此这里前面有一个负号。</p>
</li>
</ol>
<p>可以用一个实例来演示单变量函数的梯度下降：</p>
<p>$$J\left( \theta \right) =\theta ^{2}$$</p>
<p>首先对函数求导，得到：</p>
<p>$$J'\left( \theta \right) =2\theta $$</p>
<p>初始化起点，这里可以随意放置起点：</p>
<p>$$\theta ^{0}=1$$</p>
<p>学习率也可以先随意设置：</p>
<p>$$\alpha =0.4$$</p>
<p>那么由公式可以得到：</p>
<p>$$\theta ^{0}=1$$</p>
<p>$$\theta ^{1}=\theta ^{0}-J'\left( \theta \right) ^{0}=1-0.4\times2\times1=0.2$$</p>
<p>$$\theta ^{2}=\theta ^{1}-J'\left( \theta \right) ^{1}=0.2-0.4\times0.2\times2=0.04$$</p>
<p>$$\theta ^{3}=0.008$$</p>
<p>$$\theta ^{4}=0.0016$$</p>
<p>$$\theta ^{5}=0.00032$$</p>
<p>也就是说在第四次的时候，其实已经基本到达了最低点。</p>
<p><img src="https://blogimg-1302307650.cos.ap-shanghai.myqcloud.com/20190121204511639.png" alt=""></p>
<h3 id="3算法应用">3.算法应用</h3>
<h4 id="1场景分析">1.场景分析</h4>
<p><img src="https://blogimg-1302307650.cos.ap-shanghai.myqcloud.com/20190122144234426.png" alt=""></p>
<p>对于一组采样点，我们想要找到其中的规律，需要采用拟合曲线的方式。我们通过观察，可以看出近似于一条直线$y=wx+b$，想要知道这方程我们就需要求解里面的$w$和$b$，利用初高中知识我们就知道，只要有两组数据$\left( x_{1},y_{1}\right)$，$\left( x_{2},y_{2}\right)$我们就可以通过消元法来求解$w$和$b$，但是对于真实的数据来说，每个采样点可能都混入了噪声，也就是说，以采样点$\left( x_{1},y_{1}\right)$为例，存在$y_{1}=wx_{1}+b+\varepsilon $，这里的$\varepsilon $就是混入的噪声，而且$\varepsilon $的值也是不同的，那我们就不能直接利用消元法求解，而是要找到一个近似的曲线，使这个曲线相对于整个采样点来说是误差最小，最能反映数据规律的。</p>
<p>根据我们所学的梯度下降算法，我们需要将求解$y=wx+b$这个问题转化为求最小值问题，这就属于数学中的最小二乘法，在这个问题中，均方差公式可以写成：</p>
<p>$$loss = \sum\left( wx_{i}+b-y_{i}\right) ^{2}$$</p>
<p>其中$loss$是损失函数，而我们需要的就是使损失函数做到最小，这就变成了一个变量为$w$和$b$的$loss$函数求解最小值，就可以使用我们的梯度下降算法。但是对于这种比较复杂的函数，我们不可能也没必要自己手动来算，借助于我们的计算机可以更快更准确求解。</p>
<h4 id="2求解思路">2.求解思路</h4>
<p>我们已经学会了关于一个未知数的梯度下降算法，现在损失函数中有两个未知数$w$和$b$，我们应该怎么使用梯度下降算法呢？</p>
<p><img src="https://blogimg-1302307650.cos.ap-shanghai.myqcloud.com/image-20200623103141946.png" alt="image-20200623103141946"></p>
<p><img src="https://blogimg-1302307650.cos.ap-shanghai.myqcloud.com/image-20200623104321278.png" alt="image-20200623104321278"></p>
<h4 id="3代码实现">3.代码实现</h4>
<p>对于$loss = \sum\left( wx_{i}+b-y_{i}\right) ^{2}$，首先我们我们需要把具体表达式写出来，也就是对于一个数据集，$loss$可以表示为所有$\left( wx_{i}+b-y_{i}\right) ^{2}$之和，我们可以定义一个函数来表示：</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">def</span> <span style="color:#50fa7b">compute_error_for_line_given_points</span>(b,w,points):
	totalError <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">0</span>
    <span style="color:#ff79c6">for</span> i <span style="color:#ff79c6">in</span> <span style="color:#8be9fd;font-style:italic">range</span>(<span style="color:#bd93f9">0</span>,<span style="color:#8be9fd;font-style:italic">len</span>(points)):
        x <span style="color:#ff79c6">=</span> points[i,<span style="color:#bd93f9">0</span>]
        y <span style="color:#ff79c6">=</span> points[i,<span style="color:#bd93f9">1</span>]
        totalError <span style="color:#ff79c6">+=</span> (y<span style="color:#ff79c6">-</span>(w<span style="color:#ff79c6">*</span>x<span style="color:#ff79c6">+</span>b))<span style="color:#ff79c6">**</span><span style="color:#bd93f9">2</span>
     <span style="color:#ff79c6">return</span> totalError <span style="color:#ff79c6">/</span> <span style="color:#8be9fd;font-style:italic">float</span>(<span style="color:#8be9fd;font-style:italic">len</span>(points))
</code></pre></td></tr></table>
</div>
</div><p>然后计算梯度信息，二元函数和一元函数不同，一元函数的梯度就是导数，二元函数的梯度是两个偏导组成的向量，这个向量的反方向就指向了向最小值走的方向。</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">def</span> <span style="color:#50fa7b">step_gradient</span>(b_current, w_current, points, learningRate):
    b_gradient <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">0</span>
    w_gradient <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">0</span>
    N <span style="color:#ff79c6">=</span> <span style="color:#8be9fd;font-style:italic">float</span>(<span style="color:#8be9fd;font-style:italic">len</span>(points))
    <span style="color:#ff79c6">for</span> i <span style="color:#ff79c6">in</span> <span style="color:#8be9fd;font-style:italic">range</span>(<span style="color:#bd93f9">0</span>,<span style="color:#8be9fd;font-style:italic">len</span>(points)):
        x <span style="color:#ff79c6">=</span> points[i,<span style="color:#bd93f9">0</span>]
        y <span style="color:#ff79c6">=</span> points[i,<span style="color:#bd93f9">1</span>]
        b_gradient <span style="color:#ff79c6">+=</span> <span style="color:#ff79c6">-</span>(<span style="color:#bd93f9">2</span><span style="color:#ff79c6">/</span>N) <span style="color:#ff79c6">*</span> (y<span style="color:#ff79c6">-</span>((w_current<span style="color:#ff79c6">*</span>x)<span style="color:#ff79c6">+</span>b_current))
        w_gradient <span style="color:#ff79c6">+=</span> <span style="color:#ff79c6">-</span>(<span style="color:#bd93f9">2</span><span style="color:#ff79c6">/</span>N) <span style="color:#ff79c6">*</span> x<span style="color:#ff79c6">*</span>(y<span style="color:#ff79c6">-</span>((w_current<span style="color:#ff79c6">*</span>x)<span style="color:#ff79c6">+</span>b_current))
    new_b <span style="color:#ff79c6">=</span> b_current <span style="color:#ff79c6">-</span> (learningRate <span style="color:#ff79c6">*</span> b_gradient)
    new_w <span style="color:#ff79c6">=</span> w_current <span style="color:#ff79c6">-</span> (learningRate <span style="color:#ff79c6">*</span> w_gradient)
    <span style="color:#ff79c6">return</span> [new_b, new_w]
</code></pre></td></tr></table>
</div>
</div><p>最后循环迭代梯度信息：</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">def</span>  <span style="color:#50fa7b">gradient_descent_runner</span>(points, starting_b, starting_w, learning_rate, num_iterations):
    b<span style="color:#ff79c6">=</span> starting_b
    w<span style="color:#ff79c6">=</span>starting_m
    <span style="color:#ff79c6">for</span> i <span style="color:#ff79c6">in</span> <span style="color:#8be9fd;font-style:italic">range</span>(num_iterations):
        b,m <span style="color:#ff79c6">=</span> step_gradient(b,m,np<span style="color:#ff79c6">.</span>array(points),learning_rate)
     <span style="color:#ff79c6">return</span> [b,m]
</code></pre></td></tr></table>
</div>
</div><p>再写一个执行函数包一下：</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">def</span> <span style="color:#50fa7b">run</span>():
    points <span style="color:#ff79c6">=</span> np<span style="color:#ff79c6">.</span>genfromtxt(<span style="color:#f1fa8c">&#34;data.csv&#34;</span>, delimiter<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;,&#34;</span>)
    learning_rate <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">0.0001</span>
    initial_b <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">0</span>
    initial_w <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">0</span>
    num_iterations <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">100</span>
    <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;Starting gradient descent at b = </span><span style="color:#f1fa8c">{0}</span><span style="color:#f1fa8c">, m = </span><span style="color:#f1fa8c">{1}</span><span style="color:#f1fa8c">, error = </span><span style="color:#f1fa8c">{2}</span><span style="color:#f1fa8c">&#34;</span><span style="color:#ff79c6">.</span>format(initial_b, initial_w, compute_error_for_line_given_points(initial_b, initial_w, points)))
    <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;running&#34;</span>)
    [b, w] <span style="color:#ff79c6">=</span> gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations)
    <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;after </span><span style="color:#f1fa8c">{0}</span><span style="color:#f1fa8c"> iterations b = </span><span style="color:#f1fa8c">{1}</span><span style="color:#f1fa8c">, m = </span><span style="color:#f1fa8c">{2}</span><span style="color:#f1fa8c">, error = </span><span style="color:#f1fa8c">{3}</span><span style="color:#f1fa8c">&#34;</span><span style="color:#ff79c6">.</span>format(num_iterations, b, w, compute_error_for_line_given_points(b, w, points)))
</code></pre></td></tr></table>
</div>
</div><p>在根目录下写一个数据data.csv，为了方便先写4条数据试试：</p>
<p><code>data.csv</code></p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">56.130388816875467,85.498067778840223
60.269214393997906,70.251934419771587
53.66093226167304,63.642398775657753
38.954769073377065,44.847124242467601
</code></pre></td></tr></table>
</div>
</div><p>运行py程序，得到如下输出：</p>
<p><img src="https://blogimg-1302307650.cos.ap-shanghai.myqcloud.com/image-20200619105318255.png" alt="image-20200619105318255"></p>
<p>最后放一下完整代码：</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">import</span> numpy <span style="color:#ff79c6">as</span> np


<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">compute_error_for_line_given_points</span>(b, w, points):
    totalError <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">0</span>
    <span style="color:#ff79c6">for</span> i <span style="color:#ff79c6">in</span> <span style="color:#8be9fd;font-style:italic">range</span>(<span style="color:#bd93f9">0</span>, <span style="color:#8be9fd;font-style:italic">len</span>(points)):
        x <span style="color:#ff79c6">=</span> points[i, <span style="color:#bd93f9">0</span>]
        y <span style="color:#ff79c6">=</span> points[i, <span style="color:#bd93f9">1</span>]
        totalError <span style="color:#ff79c6">+=</span> (y<span style="color:#ff79c6">-</span>(w <span style="color:#ff79c6">*</span> x <span style="color:#ff79c6">+</span> b)) <span style="color:#ff79c6">**</span> <span style="color:#bd93f9">2</span>
    <span style="color:#ff79c6">return</span> totalError <span style="color:#ff79c6">/</span> <span style="color:#8be9fd;font-style:italic">float</span>(<span style="color:#8be9fd;font-style:italic">len</span>(points))


<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">step_gradient</span>(b_current, w_current, points, learningRate):
    b_gradient <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">0</span>
    w_gradient <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">0</span>
    N <span style="color:#ff79c6">=</span> <span style="color:#8be9fd;font-style:italic">float</span>(<span style="color:#8be9fd;font-style:italic">len</span>(points))
    <span style="color:#ff79c6">for</span> i <span style="color:#ff79c6">in</span> <span style="color:#8be9fd;font-style:italic">range</span>(<span style="color:#bd93f9">0</span>, <span style="color:#8be9fd;font-style:italic">len</span>(points)):
        x <span style="color:#ff79c6">=</span> points[i, <span style="color:#bd93f9">0</span>]
        y <span style="color:#ff79c6">=</span> points[i, <span style="color:#bd93f9">1</span>]
        b_gradient <span style="color:#ff79c6">+=</span> <span style="color:#ff79c6">-</span>(<span style="color:#bd93f9">2</span><span style="color:#ff79c6">/</span>N) <span style="color:#ff79c6">*</span> (y<span style="color:#ff79c6">-</span>((w_current <span style="color:#ff79c6">*</span> x) <span style="color:#ff79c6">+</span> b_current))
        w_gradient <span style="color:#ff79c6">+=</span> <span style="color:#ff79c6">-</span>(<span style="color:#bd93f9">2</span><span style="color:#ff79c6">/</span>N) <span style="color:#ff79c6">*</span> x<span style="color:#ff79c6">*</span>(y<span style="color:#ff79c6">-</span>((w_current <span style="color:#ff79c6">*</span> x) <span style="color:#ff79c6">+</span> b_current))
    new_b <span style="color:#ff79c6">=</span> b_current <span style="color:#ff79c6">-</span> (learningRate <span style="color:#ff79c6">*</span> b_gradient)
    new_w <span style="color:#ff79c6">=</span> w_current <span style="color:#ff79c6">-</span> (learningRate <span style="color:#ff79c6">*</span> w_gradient)
    <span style="color:#ff79c6">return</span> [new_b, new_w]


<span style="color:#ff79c6">def</span>  <span style="color:#50fa7b">gradient_descent_runner</span>(points, starting_b, starting_w, learning_rate, num_iterations):
    b<span style="color:#ff79c6">=</span> starting_b
    w<span style="color:#ff79c6">=</span> starting_w
    <span style="color:#ff79c6">for</span> i <span style="color:#ff79c6">in</span> <span style="color:#8be9fd;font-style:italic">range</span>(num_iterations):
        b, w <span style="color:#ff79c6">=</span> step_gradient(b, w, np<span style="color:#ff79c6">.</span>array(points), learning_rate)
    <span style="color:#ff79c6">return</span> [b, w]


<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">run</span>():
    points <span style="color:#ff79c6">=</span> np<span style="color:#ff79c6">.</span>genfromtxt(<span style="color:#f1fa8c">&#34;data.csv&#34;</span>, delimiter<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;,&#34;</span>)
    learning_rate <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">0.0001</span>
    initial_b <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">0</span>
    initial_w <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">0</span>
    num_iterations <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">100</span>
    <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;Starting gradient descent at b = </span><span style="color:#f1fa8c">{0}</span><span style="color:#f1fa8c">, m = </span><span style="color:#f1fa8c">{1}</span><span style="color:#f1fa8c">, error = </span><span style="color:#f1fa8c">{2}</span><span style="color:#f1fa8c">&#34;</span><span style="color:#ff79c6">.</span>format(initial_b, initial_w, compute_error_for_line_given_points(initial_b, initial_w, points)))
    <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;running&#34;</span>)
    [b, w] <span style="color:#ff79c6">=</span> gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations)
    <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;after </span><span style="color:#f1fa8c">{0}</span><span style="color:#f1fa8c"> iterations b = </span><span style="color:#f1fa8c">{1}</span><span style="color:#f1fa8c">, m = </span><span style="color:#f1fa8c">{2}</span><span style="color:#f1fa8c">, error = </span><span style="color:#f1fa8c">{3}</span><span style="color:#f1fa8c">&#34;</span><span style="color:#ff79c6">.</span>format(num_iterations, b, w, compute_error_for_line_given_points(b, w, points)))


<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#39;__main__&#39;</span>:
    run()

</code></pre></td></tr></table>
</div>
</div><p>参考：</p>
<p><a href="https://blog.csdn.net/qq_41800366/article/details/86583789">https://blog.csdn.net/qq_41800366/article/details/86583789</a></p>

      </div>
      <div class="footer">
        <span><a class="category" href="https://gaoshanwomeng.github.io/%20categories/%E7%AE%97%E6%B3%95/">算法</a></span>
        <span><a class="tag" href="https://gaoshanwomeng.github.io/%20tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">梯度下降</a></span>
        
      </div>
    </div>

    <ul class="menu">
    <li class="menu-item">
        <a href="https://gaoshanwomeng.github.io/" id="back-btn">
            <i class="iconfont icon-home item-btn"></i>
        </a>
    </li>
    <li class="menu-item">
        <a href="javascript:void(0);" id="back-top-btn">
            <i class="iconfont icon-top item-btn"></i>
        </a>
    </li>
    <li class="menu-item">
        <a href="javascript:void(0);" id="switch-btn">
            <i class="iconfont icon-switch item-btn"></i>
        </a>
    </li>
    <li class="menu-item">
        <a href="javascript:void(0);" id="search-btn">
            <i class="iconfont icon-search item-btn"></i>
        </a>
    </li>
    <li class="menu-item">
        
        <a class="" href="https://gaoshanwomeng.github.io/post/vue-vuecli%E7%AE%80%E4%BB%8B/" data-tooltip="vue vuecli简介">
            <i class="iconfont icon-left item-btn"></i>
            
        </a>
    </li>
    <li class="menu-item">
        
        <a class="" href="https://gaoshanwomeng.github.io/post/pytorch%E7%AE%80%E4%BB%8B/" data-tooltip="pytorch简介">
            <i class="iconfont icon-right item-btn"></i>
            
        </a>
    </li>
</ul>

  </div>
</div>


    <div class="cover animated fadeInShow"><div class="search-container animated fadeInShow">
    <input type="search" class="docsearch-input search-input" placeholder="搜索关键词" />
    <div  id="loading" class="loading-container">
        <span>搜索中...</span>
    </div>
</div>
<script src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
<script type="text/javascript">
    docsearch({
        apiKey: 'f7f9aeb8485880cfd0e747129ea4ddf9',
        indexName: 'blogsearch',
        appId: 'SGIZDO9IMB',
        inputSelector: '.docsearch-input',
        debug: true,
        algoliaOptions: {
            hitsPerPage: 50
        },
        
        
        
        queryHook: (query) => {
            $('#loading').css('display', 'flex');
        },
        transformData: (tips) => {
            $('#loading').css('display', 'none');
        }
    })
</script>
    </div>
</body>



<script type="text/javascript" src="https://gaoshanwomeng.github.io/js/util.min.9d2fa749215ef78d508b6a6a374370dd0c897a436688df49a25abca28e8fd145d1dae787bd3cf69ffcaf64a67737f50e1aa17df9be4eddb776b39bdce4a2541f.js" integrity=""></script><script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-e/4/LvThKH1gwzXhdbY2AsjR3rm7LHWyhIG5C0jiRfn8AN2eTN5ILeztWw0H9jmN" crossorigin="anonymous"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });</script>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-168042857-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
</html>